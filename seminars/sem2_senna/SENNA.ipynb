{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENNA для задачи POS-tagging\n",
    "\n",
    "SENNA (Semantic/syntactic Extraction using a Neural Network Architecture) – архитектура нейронной сети, позволяющая достигнуть state-of-the-art результатов в нескольких задачах обработки текстов. \n",
    "\n",
    "\n",
    "#### Задача POS-tagging\n",
    "ставится как задача многоклассовой классификации: \n",
    "\n",
    "* $T$ - количество различных тегов частей речи (каждое слово $w$ относится к одному из $T$ классов)\n",
    "* для каждого слова из train формируется вектор признаков \n",
    "* NN обучается по всем векторам признаков каждого слова из train \n",
    "\n",
    "#### Подход к решению \n",
    "представлен в https://arxiv.org/pdf/1103.0398.pdf( Window approach network, раздел 3.3.1):\n",
    "1. Каждое слово представляется эмбеддингом размерности $d$;\n",
    "2. Для каждого слова формируется окно длины $k$ из $(k-1)/2$ соседних слов слева от данного слова  и $(k-1)/2$ соседних слов справа от данного слова, $k$ – нечетное. (Если для слова невозможно найти соседние слова, используется padding.)\n",
    "3. Для каждого слова формируется вектор признаков, состоящий из конкатенированных эмбеддингов слов из левого окна, данного слова и слов из правого окна. Итоговая размерность вектора признаков – $d \\times k$. Этот вектор подается на вход нейронной сети;\n",
    "4. Обучается нейронная сеть, имеющая один скрытый слой с $n_h$ нейроннами и нелинейной функцией активации $\\theta$;\n",
    "5. На выходном слое нейронной сети решается задача классификации на |T| классов. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Открытый корпус: https://github.com/dialogue-evaluation/morphoRuEval-2017/blob/master/OpenCorpora_Texts.rar\n",
    "\n",
    "Предобученные эмбеддинги Facebook: https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! wget https://www.dropbox.com/s/n5pgf9nu50jvwra/unamb_sent_14_6.conllu\n",
    "! wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install tqdm\n",
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Составляем обучающую выборку \n",
    "\n",
    "- считываем выборку \n",
    "- делим на train, test по предложениям\n",
    "- каждое предложение внутри каждого из множества разделям на слова (оставляем структуру предложения в виде list, потому что нам потребуется контекст: слова слева и справа)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'unamb_sent_14_6.conllu'\n",
    "project_path = '/content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "pos_corpus = ConllCorpusReader(project_path, fileids = path, \n",
    "                               columntypes = ['ignore', 'words', 'ignore', 'pos', 'chunk'])\n",
    "sents = list(pos_corpus.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tags = set([pos for text in sents for word, pos, chunk in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(sents, test_size=0.25)\n",
    "sent_train = # your code is here\n",
    "label_train = # your code is here\n",
    "sent_test = # your code is here\n",
    "label_test = # your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Считываем эмбеддинги\n",
    "\n",
    "Далее будем подавать эмбеддинги на вход Embedding layer в поле weights (матрица). Матрица train при этом должна быть integer-encoded (слову соответствует индекс), т.е. строка матрицы - контекст слова, для которого есть POS-метка.\n",
    "\n",
    "- будем хранить не все эмбеддинги, а только для слов, которые встречаются в train и test: надо сохранить сами эмбеддинги в матрицу (word_embeddings), и запомнить соответствие слово <-> индекс в матрице (word_2_idx)\n",
    "- будьте внимательны с кодировкой .de(en)code('utf-8')\n",
    "- не забудем о PADDING, UNKNOWN (для некоторых слов не существует контекста, в этом случае эмбеддинг будет из нулей; для некоторых слов не найдется предобученного эмбеддинга, создадим для таких слов эмбеддинг np.random.uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# чтобы знать, какие слова есть\n",
    "words = set()\n",
    "\n",
    "for sent_set in [sent_train, sent_test]:\n",
    "    for sentence in sent_set:\n",
    "        for token in sentence:\n",
    "            words.add(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "word_2_idx = {}\n",
    "word_embeddings = []\n",
    "\n",
    "with open('wiki.ru.vec', 'rb') as f :\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        if len(values) != 301:\n",
    "            continue\n",
    "            \n",
    "        if len(word_2_idx) == 0:\n",
    "            \n",
    "            # your code is here\n",
    "            word_2_idx[\"padding\"] = \n",
    "            word_embeddings.append(vector)\n",
    "            \n",
    "            # your code is here\n",
    "            word_2_idx[\"unknown\"] = len(word_2_idx)\n",
    "            word_embeddings.append(vector)\n",
    "\n",
    "        if  values[0].lower().decode('utf-8') in words:\n",
    "            # your code is here\n",
    "            word_2_idx[values[0].lower()] = \n",
    "            word_embeddings.append(vector)\n",
    "            \n",
    "            \n",
    "            \n",
    "word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings.shape, len(word_2_idx), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Составляем train\n",
    "\n",
    "- сформируем окно для каждого слова размера $k$;\n",
    "- закодируем каждое слово из контекста индексом, соответсвующим этому слову в матрице эмбеддингов\n",
    "- не забываем про padding, unknown\n",
    "\n",
    "### 4. Составляем test\n",
    "- кодируем каждый label индексом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOWSIZE = 5\n",
    "UNKNOWN_IDX = word_2_idx['unknown']\n",
    "PADDING_IDX = word_2_idx['padding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context(tgt_word_idx, sentence, windowsize):\n",
    "    context = []  \n",
    "    for word_position in range(tgt_word_idx - windowsize, tgt_word_idx + windowsize+1):\n",
    "        if word_position < 0 or word_position >= len(sentence):\n",
    "            # your code is here\n",
    "            \n",
    "        context.append(word)   \n",
    "    return context\n",
    "\n",
    "\n",
    "# сюда будем записывать не сами слова, а индекс эмбеддинга\n",
    "X_train = []\n",
    "\n",
    "for sentence in sent_train:\n",
    "    for tgt_word_idx in range(len(sentence)):\n",
    "        tgt_word_context = get_context(tgt_word_idx, sentence, WINDOWSIZE/2)\n",
    "        # your code is here\n",
    "        # добавляем все индексы слов из контекста, не забываем заполнять unknown ords\n",
    "        X_train.append()\n",
    "\n",
    "        \n",
    "label_2_idx = {}\n",
    "for label in pos_tags:\n",
    "    label_2_idx[label] = len(label_2_idx)\n",
    "y_train = []\n",
    "for el in label_train:\n",
    "    # your code is here\n",
    "    y_train.extend()\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Обучаем NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "n_in = X_train.shape[1] # windowsize\n",
    "n_out = len(label_2_idx) # num of labels\n",
    "\n",
    "\n",
    "# your code is here\n",
    "# trainable=False using fixed embeddings for a text input\n",
    "words_input = Input(shape=(n_in,), dtype='int32', name='words_input')\n",
    "words = Embedding()(words_input)\n",
    "words = Flatten()(words)\n",
    "\n",
    "output = Dense(64, activation=)(words)\n",
    "output = Dense(n_out, activation=)(output)\n",
    "\n",
    "model = Model(input=[words_input], output=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, nb_epoch= 2, batch_size = 32,  validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Делаем предсказание для test\n",
    "- составляем test\n",
    "- применяем модель\n",
    "- смотрим на качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "# your code is here\n",
    "       \n",
    "y_test = []\n",
    "# your code is here\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_2_label = {v: k for k, v in label_2_idx.items()}\n",
    "pred = model.predict(X_test)\n",
    "t = np.array([idx_2_label[i] for i in y_test])\n",
    "p = np.array([idx_2_label[i] for i in np.argmax(pred, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print np.mean(t==p)\n",
    "print classification_report(t, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
