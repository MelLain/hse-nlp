{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENNA для задачи POS-tagging\n",
    "\n",
    "SENNA (Semantic/syntactic Extraction using a Neural Network Architecture) – архитектура нейронной сети, позволяющая достигнуть state-of-the-art результатов в нескольких задачах обработки текстов. \n",
    "\n",
    "\n",
    "#### Задача POS-tagging\n",
    "ставится как задача многоклассовой классификации: \n",
    "\n",
    "* $T$ - количество различных тегов частей речи (каждое слово $w$ относится к одному из $T$ классов)\n",
    "* для каждого слова из train формируется вектор признаков \n",
    "* NN обучается по всем векторам признаков каждого слова из train \n",
    "\n",
    "#### Подход к решению \n",
    "представлен в https://arxiv.org/pdf/1103.0398.pdf( Window approach network, раздел 3.3.1):\n",
    "1. Каждое слово представляется эмбеддингом размерности $d$;\n",
    "2. Для каждого слова формируется окно длины $k$ из $(k-1)/2$ соседних слов слева от данного слова  и $(k-1)/2$ соседних слов справа от данного слова, $k$ – нечетное. (Если для слова невозможно найти соседние слова, используется padding.)\n",
    "3. Для каждого слова формируется вектор признаков, состоящий из конкатенированных эмбеддингов слов из левого окна, данного слова и слов из правого окна. Итоговая размерность вектора признаков – $d \\times k$. Этот вектор подается на вход нейронной сети;\n",
    "4. Обучается нейронная сеть, имеющая один скрытый слой с $n_h$ нейроннами и нелинейной функцией активации $\\theta$;\n",
    "5. На выходном слое нейронной сети решается задача классификации на |T| классов. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Открытый корпус: https://github.com/dialogue-evaluation/morphoRuEval-2017/blob/master/OpenCorpora_Texts.rar\n",
    "\n",
    "Предобученные эмбеддинги Facebook: https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! wget https://www.dropbox.com/s/n5pgf9nu50jvwra/unamb_sent_14_6.conllu\n",
    "! wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install tqdm\n",
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Составляем обучающую выборку \n",
    "\n",
    "- считываем выборку \n",
    "- делим на train, test по предложениям\n",
    "- каждое предложение внутри каждого из множества разделям на слова (оставляем структуру предложения в виде list, потому что нам потребуется контекст: слова слева и справа)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'unamb_sent_14_6.conllu'\n",
    "project_path = '/home/nkozlovskaya/nlp_course'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "pos_corpus = ConllCorpusReader(project_path, fileids = path, \n",
    "                               columntypes = ['ignore', 'words', 'ignore', 'pos', 'chunk'])\n",
    "sents = list(pos_corpus.iob_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tags = set([pos for text in sents for word, pos, chunk in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(sents, test_size=0.25)\n",
    "sent_train = [[word[0].lower() for word in sent]for sent in train]\n",
    "label_train = [[word[1] for word in sent] for sent in train]\n",
    "sent_test = [[word[0].lower() for word in sent]for sent in test]\n",
    "label_test = [[word[1] for word in sent] for sent in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Считываем эмбеддинги\n",
    "\n",
    "Далее будем подавать эмбеддинги на вход Embedding layer в поле weights (матрица). Матрица train при этом должна быть integer-encoded (слову соответствует индекс), т.е. строка матрицы - контекст слова, для которого есть POS-метка.\n",
    "\n",
    "- будем хранить не все эмбеддинги, а только для слов, которые встречаются в train и test: надо сохранить сами эмбеддинги в матрицу (word_embeddings), и запомнить соответствие слово <-> индекс в матрице (word_2_idx)\n",
    "- будьте внимательны с кодировкой .de(en)code('utf-8')\n",
    "- не забудем о PADDING, UNKNOWN (для некоторых слов не существует контекста, в этом случае эмбеддинг будет из нулей; для некоторых слов не найдется предобученного эмбеддинга, создадим для таких слов эмбеддинг np.random.uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# чтобы знать, какие слова есть\n",
    "words = set()\n",
    "\n",
    "for sent_set in [sent_train, sent_test]:\n",
    "    for sentence in sent_set:\n",
    "        for token in sentence:\n",
    "            words.add(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888424it [00:38, 48518.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "word_2_idx = {}\n",
    "word_embeddings = []\n",
    "with open('wiki.ru.vec', 'rb') as f :\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        if len(values) != 301:\n",
    "            continue\n",
    "            \n",
    "        if len(word_2_idx) == 0:\n",
    "            word_2_idx[\"padding\"] = len(word_2_idx)\n",
    "            vector = np.zeros(len(values)-1) \n",
    "            word_embeddings.append(vector)\n",
    "\n",
    "            word_2_idx[\"unknown\"] = len(word_2_idx)\n",
    "            vector = np.random.uniform(-0.25, 0.25, len(values)-1)\n",
    "            word_embeddings.append(vector)\n",
    "            values = line.split()\n",
    "        if  values[0].lower().decode('utf-8') in words:\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_embeddings.append(vector)\n",
    "            word_2_idx[values[0].lower()] = len(word_2_idx)\n",
    "            \n",
    "            \n",
    "word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70789, 300), 70789, 79791)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape, len(word_2_idx), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Составляем train\n",
    "\n",
    "- сформируем окно для каждого слова размера $k$;\n",
    "- закодируем каждое слово из контекста индексом, соответсвующим этому слову в матрице эмбеддингов\n",
    "- не забываем про padding, unknown\n",
    "\n",
    "### 4. Составляем test\n",
    "- кодируем каждый label индексом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOWSIZE = 5\n",
    "UNKNOWN_IDX = word_2_idx['unknown']\n",
    "PADDING_IDX = word_2_idx['padding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context(tgt_word_idx, sentence, windowsize):\n",
    "    context = []  \n",
    "    for word_position in range(tgt_word_idx - windowsize, tgt_word_idx + windowsize+1):\n",
    "        if word_position < 0 or word_position >= len(sentence):\n",
    "            context.append(\"padding\")\n",
    "            continue\n",
    "        word = sentence[word_position]\n",
    "        context.append(word)   \n",
    "    return context\n",
    "\n",
    "\n",
    "# сюда будем записывать не сами слова, а индекс эмбеддинга\n",
    "X_train = []\n",
    "\n",
    "for sentence in sent_train:\n",
    "    for tgt_word_idx in range(len(sentence)):\n",
    "        tgt_word_context = get_context(tgt_word_idx, sentence, WINDOWSIZE/2)\n",
    "        X_train.append([word_2_idx.get(word.encode('utf-8'), UNKNOWN_IDX) for word in tgt_word_context])\n",
    "\n",
    "        \n",
    "label_2_idx = {}\n",
    "for label in pos_tags:\n",
    "    label_2_idx[label] = len(label_2_idx)\n",
    "y_train = []\n",
    "for el in label_train:\n",
    "    y_train.extend([label_2_idx.get(label) for label in el])\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Обучаем NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "words_input (InputLayer)         (None, 5)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 5, 300)        21236700    words_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 1500)          0           embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 64)            96064       flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 14)            910         dense_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 21,333,674\n",
      "Trainable params: 96,974\n",
      "Non-trainable params: 21,236,700\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "n_in = X_train.shape[1] # windowsize\n",
    "n_out = len(label_2_idx) # num of labels\n",
    "\n",
    "# trainable=False using fixed embeddings for a text input\n",
    "words_input = Input(shape=(n_in,), dtype='int32', name='words_input')\n",
    "words = Embedding(input_dim=word_embeddings.shape[0], output_dim=word_embeddings.shape[1], \n",
    "                  weights=[word_embeddings], trainable=False)(words_input)\n",
    "words = Flatten()(words)\n",
    "\n",
    "output = Dense(64, activation='tanh')(words)\n",
    "output = Dense(n_out, activation='softmax')(output)\n",
    "\n",
    "model = Model(input=[words_input], output=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 308996 samples, validate on 34333 samples\n",
      "Epoch 1/2\n",
      "308996/308996 [==============================] - 35s - loss: 0.1114 - val_loss: 0.1808\n",
      "Epoch 2/2\n",
      "308996/308996 [==============================] - 35s - loss: 0.0977 - val_loss: 0.1884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2b3ac6510>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, nb_epoch= 2, batch_size = 32,  validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Делаем предсказание для test\n",
    "- составляем test\n",
    "- применяем модель\n",
    "- смотрим на качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "\n",
    "for sentence in sent_test:\n",
    "    for tgt_word_idx in range(len(sentence)):\n",
    "        tgt_word_context = get_context(tgt_word_idx, sentence, WINDOWSIZE/2)\n",
    "        X_test.append([word_2_idx.get(word.encode('utf-8'), UNKNOWN_IDX) for word in tgt_word_context])\n",
    "        \n",
    "y_test = []\n",
    "for el in label_test:\n",
    "    y_test.extend([label_2_idx.get(label) for label in el])\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_2_label = {v: k for k, v in label_2_idx.items()}\n",
    "pred = model.predict(X_test)\n",
    "t = np.array([idx_2_label[i] for i in y_test])\n",
    "p = np.array([idx_2_label[i] for i in np.argmax(pred, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9454636161534826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ADJ       0.96      0.91      0.94     11869\n",
      "        ADP       0.99      0.99      0.99     10726\n",
      "        ADV       0.88      0.89      0.89      3374\n",
      "       CONJ       0.96      0.98      0.97      5532\n",
      "        DET       0.96      0.95      0.96      3112\n",
      "       INTJ       0.76      0.50      0.60       122\n",
      "       NOUN       0.96      0.97      0.97     30494\n",
      "        NUM       0.87      0.79      0.83      2605\n",
      "       PART       0.95      0.89      0.92      2207\n",
      "       PRON       0.97      0.95      0.96      2221\n",
      "      PROPN       0.83      0.90      0.86      3690\n",
      "      PUNCT       0.96      0.99      0.98     22684\n",
      "       VERB       0.96      0.97      0.97     10414\n",
      "          X       0.71      0.69      0.70      5204\n",
      "\n",
      "avg / total       0.95      0.95      0.95    114254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print np.mean(t==p)\n",
    "print classification_report(t, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
