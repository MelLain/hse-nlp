{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import os\n",
    "from visualization import highlight_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать данные из корпуса новостей CNN/DailyMail.\n",
    "В рамках семинара используется подвыборка из 300 текстов CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './cnn_stories_short/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/kofxrgod7kl720m/cnn_stories_short.zip\n",
    "!mkdir cnn_data \n",
    "!unzip cnn_stories_short.zip -d $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "summaries = []\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    with open(os.path.join(DATA_DIR,filename),'r') as input_file:\n",
    "        all_texts = input_file.read().split('@highlight')\n",
    "        texts.append(all_texts[0])\n",
    "        summaries.append('. '.join(map(lambda x: x.strip(), all_texts[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нам понадобятся: \n",
    "* тексты, разбитые на предложения \n",
    "* предложения, разбитые на токены\n",
    "* тексты, разбитые предложения, которые разбиты на токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_tokenized_texts = [sent_tokenize(text) for text in texts]\n",
    "tokenized_sentences = [word_tokenize(sent) for text in texts for sent in sent_tokenize(text)]\n",
    "tokenized_texts = [[word_tokenize(sent) for sent in text] for text in sent_tokenized_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать предобученные вектора Glove. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        word_embeddings[word] = np.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе эмбеддингов слов строим векторные представления предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer:\n",
    "    \n",
    "    def __init__(self, embedding_model, dim=100):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.word2weight = None\n",
    "        self.dim = dim\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = np.max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(lambda: max_idf, [(w,tfidf.idf_[i]) for w,i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.embedding_model[w] * self.word2weight[w] \n",
    "                                  for w in words if w in self.embedding_model] or [np.zeros(self.dim)], axis=0) \n",
    "                         for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectorizer = TfidfEmbeddingVectorizer(word_embeddings)\n",
    "sentence_vectorizer = sentence_vectorizer.fit(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем один текст и построим для него матрицу расстояний. В качестве метрики используем косинусное расстояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_NUM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenized_texts[TEXT_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = sentence_vectorizer.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cosine_similarity(vectorized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summarization $-$ TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ G = (V,E) - граф $$\n",
    "$$$$\n",
    "$$ PageRank(w) = (1-d) +  d \\sum_{u} \\frac {PageRank(u)} {C(u)}$$\n",
    "\n",
    "$$u\\ -\\ вершина\\ графа,\\ такая\\ что\\ (u,w) \\in E$$\n",
    "$$$$\n",
    "$$d = 0,85\\ -\\ коэффициент\\ затухания$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank(G, s = .85, maxerr = .0001):\n",
    "    \n",
    "    n = G.shape[0]\n",
    "    A = csr_matrix(G,dtype=np.float)\n",
    "    rsums = np.array(A.sum(1))[:,0]\n",
    "    ri, ci = A.nonzero()\n",
    "    A.data /= rsums[ri]\n",
    "\n",
    "    sink = rsums==0\n",
    "    ro, r = np.zeros(n), np.ones(n)\n",
    "    while np.sum(np.abs(r-ro)) > maxerr:\n",
    "        ro = r.copy()\n",
    "        for i in range(0,n):\n",
    "            ## your\n",
    "            ## code\n",
    "            ## here\n",
    "    return r/float(sum(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = pageRank(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним нашу реализацию с реализацией NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_matrix(G)\n",
    "nx_scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_NUM\n",
    "print(\"Our implementation: {0}\\nNetworkX implementation: {1}\".format(scores[sentence_num],nx_scores[sentence_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s,i) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_LEN  = 5\n",
    "\n",
    "for i in range(SUMMARY_LEN):\n",
    "    print(' '.join(ranked_sentences[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_sentences = [sent_tokenized_texts[test_sentence_num][i] for score,sentence,i in ranked_sentences][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_sentences(sent_tokenized_texts[5],extracted_sentences)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрика качества:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Доля\\ n-грамм\\ из\\ рефератов,\\ вошедших\\ в\\ s:$\n",
    "$$$$\n",
    "$ ROUGE_n(S) = \\frac{\\sum_{r\\in R} \\sum_{w} [w \\in s][w \\in r]}{\\sum_{r \\in R} \\sum_w [w \\in r]}$ \n",
    "$$$$\n",
    "$ Доля\\ n-грамм\\ самого\\ близкого\\ реферата,\\ вошедших\\ в\\ s:$\n",
    "$$$$\n",
    "$ ROUGE_{n_{multi}}(S) = \\frac{max_{r\\in R} \\sum_{w} [w \\in s][w \\in r]}{\\sum_{r \\in R} \\sum_w [w \\in r]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your\n",
    "#code\n",
    "#here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
