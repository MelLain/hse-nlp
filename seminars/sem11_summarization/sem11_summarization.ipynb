{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import os\n",
    "from visualization import highlight_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать данные из корпуса новостей CNN/DailyMail.\n",
    "В рамках семинара используется подвыборка из 300 текстов CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './cnn_stories_short/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-05 09:39:00--  https://www.dropbox.com/s/kofxrgod7kl720m/cnn_stories_short.zip\n",
      "Распознаётся www.dropbox.com (www.dropbox.com)… 162.125.66.1\n",
      "Подключение к www.dropbox.com (www.dropbox.com)|162.125.66.1|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 301 Moved Permanently\n",
      "Адрес: /s/raw/kofxrgod7kl720m/cnn_stories_short.zip [переход]\n",
      "--2018-12-05 09:39:02--  https://www.dropbox.com/s/raw/kofxrgod7kl720m/cnn_stories_short.zip\n",
      "Повторное использование соединения с www.dropbox.com:443.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://ucc23f13e6a28dbaeae950b2809d.dl.dropboxusercontent.com/cd/0/inline/AW27OapwaPGf1PXE7miQ5g8tuUSypCsdDbwQYEPIWBvJV1zwSSajA0ZW00Hco3MDEyySQM7qfWII-qATuvmSbBq_3SWr_0cOasaoHmH8-REu31JSl-kqBbiLDWQeSOjEPYet2RYYZsrCtCSJoEoFFvm_RE2kqLRq8zC60Yd7D4d2Lqc3HLz8UKFBTTlmF8fkuC4/file [переход]\n",
      "--2018-12-05 09:39:02--  https://ucc23f13e6a28dbaeae950b2809d.dl.dropboxusercontent.com/cd/0/inline/AW27OapwaPGf1PXE7miQ5g8tuUSypCsdDbwQYEPIWBvJV1zwSSajA0ZW00Hco3MDEyySQM7qfWII-qATuvmSbBq_3SWr_0cOasaoHmH8-REu31JSl-kqBbiLDWQeSOjEPYet2RYYZsrCtCSJoEoFFvm_RE2kqLRq8zC60Yd7D4d2Lqc3HLz8UKFBTTlmF8fkuC4/file\n",
      "Распознаётся ucc23f13e6a28dbaeae950b2809d.dl.dropboxusercontent.com (ucc23f13e6a28dbaeae950b2809d.dl.dropboxusercontent.com)… 162.125.66.6\n",
      "Подключение к ucc23f13e6a28dbaeae950b2809d.dl.dropboxusercontent.com (ucc23f13e6a28dbaeae950b2809d.dl.dropboxusercontent.com)|162.125.66.6|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 FOUND\n",
      "Адрес: /cd/0/inline2/AW2vGAIWPO-30UwFkKgoHPSU6fbKFG_GQ4huUW-h-zVvLoYSUKNM06bkrzbyeWpvkhZiJh1Of42WA3q70-z4aMcc_j4aLhHmEdwCX98-E5cuUGh1PMJUeSvo3R2QucJI--mK2n4wlioO6xNAQ7jsgyTqjqD0pHaAY5cUmYimRAsZOj16craDldN76hWRACqFkRoHL5Ppgw0XaJXCPxKlYfD9N4fO4AmjAfJO_Dkn81Dr5wmSYhHEmbqppP2NAqYVK3Sg9yVe8AyRoPPrpG2ZLnPdTSmSWnS3Cpk_Z6yvEeHWoazUY5hG8LuEoA3MRvDdbkfello0UYd5b40WWL6usp5IyxxApB0mMK2T9beIdOPLqkmvssT9XMxYlyHZmDYeCbgB4UDogVKWn15RK8a7nENT-CZ0pEl7cwi9xwlXs2iF7wlDJAsXRkMYjDL7T0pvaHw/file [переход]\n",
      "--2018-12-05 09:39:03--  https://ucc23f13e6a28dbaeae950b2809d.dl.dropboxusercontent.com/cd/0/inline2/AW2vGAIWPO-30UwFkKgoHPSU6fbKFG_GQ4huUW-h-zVvLoYSUKNM06bkrzbyeWpvkhZiJh1Of42WA3q70-z4aMcc_j4aLhHmEdwCX98-E5cuUGh1PMJUeSvo3R2QucJI--mK2n4wlioO6xNAQ7jsgyTqjqD0pHaAY5cUmYimRAsZOj16craDldN76hWRACqFkRoHL5Ppgw0XaJXCPxKlYfD9N4fO4AmjAfJO_Dkn81Dr5wmSYhHEmbqppP2NAqYVK3Sg9yVe8AyRoPPrpG2ZLnPdTSmSWnS3Cpk_Z6yvEeHWoazUY5hG8LuEoA3MRvDdbkfello0UYd5b40WWL6usp5IyxxApB0mMK2T9beIdOPLqkmvssT9XMxYlyHZmDYeCbgB4UDogVKWn15RK8a7nENT-CZ0pEl7cwi9xwlXs2iF7wlDJAsXRkMYjDL7T0pvaHw/file\n",
      "Повторное использование соединения с ucc23f13e6a28dbaeae950b2809d.dl.dropboxusercontent.com:443.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 743091 (726K) [application/zip]\n",
      "Сохранение в: «cnn_stories_short.zip.9»\n",
      "\n",
      "cnn_stories_short.z 100%[===================>] 725,67K  2,17MB/s    за 0,3s    \n",
      "\n",
      "2018-12-05 09:39:04 (2,17 MB/s) - «cnn_stories_short.zip.9» сохранён [743091/743091]\n",
      "\n",
      "mkdir: cnn_data: File exists\n",
      "Archive:  cnn_stories_short.zip\n",
      "replace ./cnn_stories_short/0a9dbfea7ca576578b8b03e9a08e39c7e203ab86.story? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/kofxrgod7kl720m/cnn_stories_short.zip\n",
    "!mkdir cnn_data \n",
    "!unzip cnn_stories_short.zip -d $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "summaries = []\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    with open(os.path.join(DATA_DIR,filename),'r') as input_file:\n",
    "        all_texts = input_file.read().split('@highlight')\n",
    "        texts.append(all_texts[0])\n",
    "        summaries.append('. '.join(map(lambda x: x.strip(), all_texts[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нам понадобятся: \n",
    "* тексты, разбитые на предложения \n",
    "* предложения, разбитые на токены\n",
    "* тексты, разбитые предложения, которые разбиты на токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_tokenized_texts = [sent_tokenize(text) for text in texts]\n",
    "tokenized_sentences = [word_tokenize(sent) for text in texts for sent in sent_tokenize(text)]\n",
    "tokenized_texts = [[word_tokenize(sent) for sent in text] for text in sent_tokenized_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать предобученные вектора Glove. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-05 09:41:57--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Распознаётся nlp.stanford.edu (nlp.stanford.edu)… 171.64.67.140\n",
      "Подключение к nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://nlp.stanford.edu/data/glove.6B.zip [переход]\n",
      "--2018-12-05 09:41:57--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Подключение к nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 862182613 (822M) [application/zip]\n",
      "Сохранение в: «glove.6B.zip.1»\n",
      "\n",
      "glove.6B.zip.1      100%[===================>] 822,24M  3,34MB/s    за 3m 34s  \n",
      "\n",
      "2018-12-05 09:45:32 (3,85 MB/s) - «glove.6B.zip.1» сохранён [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        word_embeddings[word] = np.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе эмбеддингов слов строим векторные представления предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer:\n",
    "    \n",
    "    def __init__(self, embedding_model, dim=100):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.word2weight = None\n",
    "        self.dim = dim\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = np.max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(lambda: max_idf, [(w,tfidf.idf_[i]) for w,i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.embedding_model[w] * self.word2weight[w] \n",
    "                                  for w in words if w in self.embedding_model] or [np.zeros(self.dim)], axis=0) \n",
    "                         for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectorizer = TfidfEmbeddingVectorizer(word_embeddings)\n",
    "sentence_vectorizer = sentence_vectorizer.fit(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем один текст и построим для него матрицу расстояний. В качестве метрики используем косинусное расстояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_NUM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenized_texts[TEXT_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = sentence_vectorizer.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cosine_similarity(vectorized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summarization $-$ TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ G = (V,E) - граф $$\n",
    "$$$$\n",
    "$$ PageRank(w) = (1-d) +  d \\sum_{u} \\frac {PageRank(u)} {C(u)}$$\n",
    "\n",
    "$$u\\ -\\ вершина\\ графа,\\ такая\\ что\\ (u,w) \\in E$$\n",
    "$$$$\n",
    "$$d = 0,85\\ -\\ коэффициент\\ затухания$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank(G, s = .85, maxerr = .0001):\n",
    "    \n",
    "    n = G.shape[0]\n",
    "    A = csr_matrix(G,dtype=np.float)\n",
    "    rsums = np.array(A.sum(1))[:,0]\n",
    "    ri, ci = A.nonzero()\n",
    "    A.data /= rsums[ri]\n",
    "\n",
    "    sink = rsums==0\n",
    "    ro, r = np.zeros(n), np.ones(n)\n",
    "    while np.sum(np.abs(r-ro)) > maxerr:\n",
    "        ro = r.copy()\n",
    "        for i in range(0,n):\n",
    "            ## your\n",
    "            ## code\n",
    "            ## here\n",
    "    return r/float(sum(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = pageRank(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним нашу реализацию с реализацией NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_matrix(G)\n",
    "nx_scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_NUM\n",
    "print(\"Our implementation: {0}\\nNetworkX implementation: {1}\".format(scores[sentence_num],nx_scores[sentence_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s,i) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_LEN  = 5\n",
    "\n",
    "for i in range(SUMMARY_LEN):\n",
    "    print(' '.join(ranked_sentences[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_sentences = [sent_tokenized_texts[test_sentence_num][i] for score,sentence,i in ranked_sentences][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_html(sent_tokenized_text,extracted_sentences):\n",
    "    result_html = '<body>'\n",
    "    for sentence in sent_tokenized_text:\n",
    "        if sentence in extracted_sentences:\n",
    "            result_html += '<p class=\"highlighted_text\"> {0}.</p>'.format(sentence.replace('\\n', '<br>'))\n",
    "        else:\n",
    "            result_html += ' {0}'.format(sentence)#'<p class=\"raw_text\"> {0}.</p>'.format(sentence.replace('\\n', '<br>'))\n",
    "    return result_html + '</body>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_sentences(sent_tokenized_texts[5],extracted_sentences)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрика качества:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Доля\\ n-грамм\\ из\\ рефератов,\\ вошедших\\ в\\ s:$\n",
    "$$$$\n",
    "$ ROUGE_n(S) = \\frac{\\sum_{r\\in R} \\sum_{w} [w \\in s][w \\in r]}{\\sum_{r \\in R} \\sum_w [w \\in r]}$ \n",
    "$$$$\n",
    "$ Доля\\ n-грамм\\ самого\\ близкого\\ реферата,\\ вошедших\\ в\\ s:$\n",
    "$$$$\n",
    "$ ROUGE_{n_{multi}}(S) = \\frac{max_{r\\in R} \\sum_{w} [w \\in s][w \\in r]}{\\sum_{r \\in R} \\sum_w [w \\in r]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
