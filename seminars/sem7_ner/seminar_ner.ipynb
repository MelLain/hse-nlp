{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-lOKEKhNPUJ"
   },
   "source": [
    "### Семинар по NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Toynx81FNPXU"
   },
   "source": [
    "План:\n",
    "\n",
    "- Rule-based NER для русского языка с помощью Natasha\n",
    "- NER для анлгийского с помощью обученного теггера в SpaCy\n",
    "- Обучение NER на conll2003 с помощью BiLSTM+CRF и Transformer+CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UZm7gEeNPZs"
   },
   "source": [
    "Начнём с того, что скачаем и подготовим всё, что нам может потребовать в этом ноутбуке. В этот список входят:\n",
    "-  PyTorch и дополнительные библиотеки\n",
    "- natasha (https://github.com/natasha/natasha)\n",
    "- обученная модель NER для SpaCy и вспомогательная библиотека для парсинга XML\n",
    "- датасет Conll2003, который мы сохраним сразу во всех местах, нужных для библиотек, которые будут запускаться\n",
    "- репозиторий с туториалом по BiLSTM+CRF для NER на PyTorch, откуда используем соответствующую модель\n",
    "- предобученные эмбеддинги слов (GloVe) для этой модели\n",
    "- репозиторий библиотеки torchnlp, в которой есть реализации BiLSTM+CRF и Transformer+CRF для NER на PyTorch (с установкой)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNgICQc_NNxC"
   },
   "outputs": [],
   "source": [
    "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GAkN7NkQN6Bp"
   },
   "outputs": [],
   "source": [
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oaIm8aVN6Ep"
   },
   "outputs": [],
   "source": [
    "!pip3 install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCLTKBR0N6JD"
   },
   "outputs": [],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EIiWTIeN6Me"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t37nKO1VN6Pm"
   },
   "outputs": [],
   "source": [
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYKkJjxJN6Sr"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TMSQpeD3N6WW"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9iy8FQkN6Hj"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhjuWNUrOnXC"
   },
   "outputs": [],
   "source": [
    "!mkdir datasets && mv eng.* datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ya4F39X6OnZm"
   },
   "outputs": [],
   "source": [
    "!wget https://worksheets.codalab.org/rest/bundles/0x15a09c8f74f94a20bec0b68a2e6703b3/contents/blob/ && mkdir embeddings && mv index.html embeddings/glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76uT7ArkOnhW"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bXF3v64OngK"
   },
   "outputs": [],
   "source": [
    "!mkdir conll2003 && cp datasets/eng.testa conll2003/eng.testa.txt && cp datasets/eng.testb conll2003/eng.testb.txt && cp datasets/eng.train conll2003/eng.train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGm5QsgjOnc3"
   },
   "outputs": [],
   "source": [
    "!mkdir .data && mv conll2003 .data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84gbZ3ciO3Eo"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/kolloldas/torchnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldtH71ROO3Hq"
   },
   "outputs": [],
   "source": [
    "!cd torchnlp && pip install -r requirements.txt && python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6e-MuU16NPcn"
   },
   "source": [
    "После выполнение этого шага произведите рестрат ноутбука (Runtime -> Restart runtime). Почему-то библиотека torchnlp подхватывается ядром ноутбука только после перезапуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Thb2zQwREM4"
   },
   "source": [
    "Начнём с запуска Natasha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WdfHY9ORDNx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 75) Name(first='иисус', middle=None, last=None, nick=None)\n",
      "[86, 95) Name(first='мухаммед', middle=None, last=None, nick=None)\n",
      "[107, 120) Name(first='иисус', middle=None, last='христос', nick=None)\n"
     ]
    }
   ],
   "source": [
    "from natasha import NamesExtractor\n",
    "\t\n",
    "\n",
    "text = '''\n",
    "Простите, еще несколько цитат из приговора. «…Отрицал существование\n",
    "Иисуса и пророка Мухаммеда», «наделял Иисуса Христа качествами\n",
    "ожившего мертвеца — зомби» [и] «качествами покемонов —\n",
    "представителей бестиария японской мифологии, тем самым совершил\n",
    "преступление, предусмотренное статьей 148 УК РФ\n",
    "'''\n",
    "extractor = NamesExtractor()\n",
    "matches = extractor(text)\n",
    "for match in matches:\n",
    "    print(match.span, match.fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Правила для Natasha написаны на [Yargy](https://yargy.readthedocs.io/ru/latest/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Донецкая', 'народная', 'республика']\n",
      "['Чеченской', 'республике']\n"
     ]
    }
   ],
   "source": [
    "from yargy import Parser, rule, and_\n",
    "from yargy.predicates import gram, is_capitalized, dictionary\n",
    "\n",
    "\n",
    "GEO = rule(\n",
    "    and_(\n",
    "        gram('ADJF'),  # так помечается прилагательное, остальные пометки описаны в\n",
    "                       # http://pymorphy2.readthedocs.io/en/latest/user/grammemes.html\n",
    "        is_capitalized()\n",
    "    ),\n",
    "    gram('ADJF').optional().repeatable(),\n",
    "    dictionary({\n",
    "        'федерация',\n",
    "        'республика'\n",
    "    })\n",
    ")\n",
    "\n",
    "\n",
    "parser = Parser(GEO)\n",
    "text = '''\n",
    "В Чеченской республике на день рождения ...\n",
    "Донецкая народная республика провозгласила ...\n",
    "Башня Федерация — одна из самых высоких ...\n",
    "'''\n",
    "for match in parser.findall(text):\n",
    "    print([_.value for _ in match.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIZVICAdRQoJ"
   },
   "source": [
    "**Задание: ** выберите несколько типов именованных сущностей и попробуйте придумать несколько правил для каждого из них. Напишите функцию, которая на основании этих правил будет искать в подаваемом тексте сущности нужных типов. Опробуйте на нескольких случайных документах из Интернета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0OCLAxCRlEJ"
   },
   "source": [
    "Теперь применим готовую модель для английского языка с помощью SpaCy (статья https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkDsM-H9RDX6"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5a5LYVFpR8t5"
   },
   "source": [
    "Выкачаем статью и найдём в ней именованные сущности, выведем их число:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkocIBlMRDVw"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "def url_to_string(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "ny_bb = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')\n",
    "article = nlp(ny_bb)\n",
    "len(article.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7obkmM8JSLKC"
   },
   "source": [
    "Выведем число встреченных сущностей каждого типа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thgvvhtpRDTW"
   },
   "outputs": [],
   "source": [
    "labels = [x.label_ for x in article.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55YPeDK6ShXX"
   },
   "source": [
    "Выведем текст с подсвеченными сущностями разных типов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bA7nfLkgRDRX"
   },
   "outputs": [],
   "source": [
    "sentences = [x for x in article.sents]\n",
    "displacy.render(nlp(str(sentences)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gudee6_zSyjQ"
   },
   "source": [
    "Перейдём к обучению своих моделей. Воспользуемся кодом из туториала:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b97MvhwvTPgl"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('./a-PyTorch-Tutorial-to-Sequence-Labeling')\n",
    "\n",
    "from models import LM_LSTM_CRF, ViterbiLoss\n",
    "from utils import *\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from datasets import WCDataset\n",
    "from inference import ViterbiDecoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7Z65gh5TPdi"
   },
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "task = 'ner'  # tagging task, to choose column in CoNLL 2003 dataset\n",
    "train_file = './datasets/eng.train'  # path to training data\n",
    "val_file = './datasets/eng.testa'  # path to validation data\n",
    "test_file = './datasets/eng.testb'  # path to test data\n",
    "emb_file = './embeddings/glove.6B.100d.txt'  # path to pre-trained word embeddings\n",
    "min_word_freq = 5  # threshold for word frequency\n",
    "min_char_freq = 1  # threshold for character frequency\n",
    "caseless = True  # lowercase everything?\n",
    "expand_vocab = True  # expand model's input vocabulary to the pre-trained embeddings' vocabulary?\n",
    "\n",
    "# Model parameters\n",
    "char_emb_dim = 30  # character embedding size\n",
    "with open(emb_file, 'r') as f:\n",
    "    word_emb_dim = len(f.readline().split(' ')) - 1  # word embedding size\n",
    "word_rnn_dim = 300  # word RNN size\n",
    "char_rnn_dim = 300  # character RNN size\n",
    "char_rnn_layers = 1  # number of layers in character RNN\n",
    "word_rnn_layers = 1  # number of layers in word RNN\n",
    "highway_layers = 1  # number of layers in highway network\n",
    "dropout = 0.5  # dropout\n",
    "fine_tune_word_embeddings = False  # fine-tune pre-trained word embeddings?\n",
    "\n",
    "# Training parameters\n",
    "start_epoch = 0  # start at this epoch\n",
    "batch_size = 10  # batch size\n",
    "lr = 0.015  # learning rate\n",
    "lr_decay = 0.05  # decay learning rate by this amount\n",
    "momentum = 0.9  # momentum\n",
    "workers = 1  # number of workers for loading data in the DataLoader\n",
    "epochs = 10  # number of epochs to run without early-stopping\n",
    "grad_clip = 5.  # clip gradients at this value\n",
    "print_freq = 100  # print training or validation status every __ batches\n",
    "best_f1 = 0.  # F1 score to start with\n",
    "checkpoint = None  # path to model checkpoint, None if none\n",
    "\n",
    "tag_ind = 1 if task == 'pos' else 3  # choose column in CoNLL 2003 dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "416h6eAZTPbU"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder):\n",
    "    \"\"\"\n",
    "    Performs one epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param lm_criterion: cross entropy loss layer\n",
    "    :param crf_criterion: viterbi loss layer\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    :param vb_decoder: viterbi decoder (to decode and find F1 score)\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n",
    "    data_time = AverageMeter()  # data loading time per batch\n",
    "    ce_losses = AverageMeter()  # cross entropy loss\n",
    "    vb_losses = AverageMeter()  # viterbi loss\n",
    "    f1s = AverageMeter()  # f1 score\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n",
    "            train_loader):\n",
    "\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        max_word_len = max(wmap_lengths.tolist())\n",
    "        max_char_len = max(cmap_lengths.tolist())\n",
    "\n",
    "        # Reduce batch's padded length to maximum in-batch sequence\n",
    "        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
    "        wmaps = wmaps[:, :max_word_len].to(device)\n",
    "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
    "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
    "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
    "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
    "        tmaps = tmaps[:, :max_word_len].to(device)\n",
    "        wmap_lengths = wmap_lengths.to(device)\n",
    "        cmap_lengths = cmap_lengths.to(device)\n",
    "\n",
    "        # Forward prop.\n",
    "        crf_scores, lm_f_scores, lm_b_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
    "                                                                                                             cmaps_b,\n",
    "                                                                                                             cmarkers_f,\n",
    "                                                                                                             cmarkers_b,\n",
    "                                                                                                             wmaps,\n",
    "                                                                                                             tmaps,\n",
    "                                                                                                             wmap_lengths,\n",
    "                                                                                                             cmap_lengths)\n",
    "\n",
    "        # LM loss\n",
    "\n",
    "        # We don't predict the next word at the pads or <end> tokens\n",
    "        # We will only predict at [dunston, checks, in] among [dunston, checks, in, <end>, <pad>, <pad>, ...]\n",
    "        # So, prediction lengths are word sequence lengths - 1\n",
    "        lm_lengths = wmap_lengths_sorted - 1\n",
    "        lm_lengths = lm_lengths.tolist()\n",
    "\n",
    "        # Remove scores at timesteps we won't predict at\n",
    "        # pack_padded_sequence is a good trick to do this (see dynamic_rnn.py, where we explore this)\n",
    "        lm_f_scores, _ = pack_padded_sequence(lm_f_scores, lm_lengths, batch_first=True)\n",
    "        lm_b_scores, _ = pack_padded_sequence(lm_b_scores, lm_lengths, batch_first=True)\n",
    "\n",
    "        # For the forward sequence, targets are from the second word onwards, up to <end>\n",
    "        # (timestep -> target) ...dunston -> checks, ...checks -> in, ...in -> <end>\n",
    "        lm_f_targets = wmaps_sorted[:, 1:]\n",
    "        lm_f_targets, _ = pack_padded_sequence(lm_f_targets, lm_lengths, batch_first=True)\n",
    "\n",
    "        # For the backward sequence, targets are <end> followed by all words except the last word\n",
    "        # ...notsnud -> <end>, ...skcehc -> dunston, ...ni -> checks\n",
    "        lm_b_targets = torch.cat(\n",
    "            [torch.LongTensor([word_map['<end>']] * wmaps_sorted.size(0)).unsqueeze(1).to(device), wmaps_sorted], dim=1)\n",
    "        lm_b_targets, _ = pack_padded_sequence(lm_b_targets, lm_lengths, batch_first=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        ce_loss = lm_criterion(lm_f_scores, lm_f_targets) + lm_criterion(lm_b_scores, lm_b_targets)\n",
    "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
    "        loss = ce_loss + vb_loss\n",
    "\n",
    "        # Back prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Viterbi decode to find accuracy / f1\n",
    "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
    "\n",
    "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
    "        decoded, _ = pack_padded_sequence(decoded, lm_lengths, batch_first=True)\n",
    "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
    "        tmaps_sorted, _ = pack_padded_sequence(tmaps_sorted, lm_lengths, batch_first=True)\n",
    "\n",
    "        # F1\n",
    "        f1 = f1_score(tmaps_sorted.to(\"cpu\").numpy(), decoded.numpy(), average='macro')\n",
    "\n",
    "        # Keep track of metrics\n",
    "        ce_losses.update(ce_loss.item(), sum(lm_lengths))\n",
    "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "        f1s.update(f1, sum(lm_lengths))\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print training status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'CE Loss {ce_loss.val:.4f} ({ce_loss.avg:.4f})\\t'\n",
    "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
    "                  'F1 {f1.val:.3f} ({f1.avg:.3f})'.format(epoch, i, len(train_loader),\n",
    "                                                          batch_time=batch_time,\n",
    "                                                          data_time=data_time, ce_loss=ce_losses,\n",
    "                                                          vb_loss=vb_losses, f1=f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7offGaETPY0"
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, crf_criterion, vb_decoder):\n",
    "    \"\"\"\n",
    "    Performs one epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data\n",
    "    :param model: model\n",
    "    :param crf_criterion: viterbi loss layer\n",
    "    :param vb_decoder: viterbi decoder\n",
    "    :return: validation F1 score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    vb_losses = AverageMeter()\n",
    "    f1s = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n",
    "            val_loader):\n",
    "\n",
    "        max_word_len = max(wmap_lengths.tolist())\n",
    "        max_char_len = max(cmap_lengths.tolist())\n",
    "\n",
    "        # Reduce batch's padded length to maximum in-batch sequence\n",
    "        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
    "        wmaps = wmaps[:, :max_word_len].to(device)\n",
    "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
    "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
    "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
    "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
    "        tmaps = tmaps[:, :max_word_len].to(device)\n",
    "        wmap_lengths = wmap_lengths.to(device)\n",
    "        cmap_lengths = cmap_lengths.to(device)\n",
    "\n",
    "        # Forward prop.\n",
    "        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
    "                                                                                   cmaps_b,\n",
    "                                                                                   cmarkers_f,\n",
    "                                                                                   cmarkers_b,\n",
    "                                                                                   wmaps,\n",
    "                                                                                   tmaps,\n",
    "                                                                                   wmap_lengths,\n",
    "                                                                                   cmap_lengths)\n",
    "\n",
    "        # Viterbi / CRF layer loss\n",
    "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
    "\n",
    "        # Viterbi decode to find accuracy / f1\n",
    "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
    "\n",
    "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
    "        decoded, _ = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
    "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
    "        tmaps_sorted, _ = pack_padded_sequence(tmaps_sorted, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
    "\n",
    "        # f1\n",
    "        f1 = f1_score(tmaps_sorted.to(\"cpu\").numpy(), decoded.numpy(), average='macro')\n",
    "\n",
    "        # Keep track of metrics\n",
    "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
    "        f1s.update(f1, sum((wmap_lengths_sorted - 1).tolist()))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Validation: [{0}/{1}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
    "                  'F1 Score {f1.val:.3f} ({f1.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
    "                                                                  vb_loss=vb_losses, f1=f1s))\n",
    "\n",
    "    print(\n",
    "        '\\n * LOSS - {vb_loss.avg:.3f}, F1 SCORE - {f1.avg:.3f}\\n'.format(vb_loss=vb_losses,\n",
    "                                                                          f1=f1s))\n",
    "\n",
    "    return f1s.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cw_wP_AMTj7g"
   },
   "outputs": [],
   "source": [
    "def main_func():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "    global best_f1, epochs_since_improvement, checkpoint, start_epoch, word_map, char_map, tag_map\n",
    "\n",
    "    # Read training and validation data\n",
    "    train_words, train_tags = read_words_tags(train_file, tag_ind, caseless)\n",
    "    val_words, val_tags = read_words_tags(val_file, tag_ind, caseless)\n",
    "\n",
    "    # Initialize model or load checkpoint\n",
    "    if checkpoint is not None:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        model = checkpoint['model']\n",
    "        optimizer = checkpoint['optimizer']\n",
    "        word_map = checkpoint['word_map']\n",
    "        lm_vocab_size = checkpoint['lm_vocab_size']\n",
    "        tag_map = checkpoint['tag_map']\n",
    "        char_map = checkpoint['char_map']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_f1 = checkpoint['f1']\n",
    "    else:\n",
    "        word_map, char_map, tag_map = create_maps(train_words + val_words, train_tags + val_tags, min_word_freq,\n",
    "                                                  min_char_freq)  # create word, char, tag maps\n",
    "        embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map,\n",
    "                                                              expand_vocab)  # load pre-trained embeddings\n",
    "\n",
    "        model = LM_LSTM_CRF(tagset_size=len(tag_map),\n",
    "                            charset_size=len(char_map),\n",
    "                            char_emb_dim=char_emb_dim,\n",
    "                            char_rnn_dim=char_rnn_dim,\n",
    "                            char_rnn_layers=char_rnn_layers,\n",
    "                            vocab_size=len(word_map),\n",
    "                            lm_vocab_size=lm_vocab_size,\n",
    "                            word_emb_dim=word_emb_dim,\n",
    "                            word_rnn_dim=word_rnn_dim,\n",
    "                            word_rnn_layers=word_rnn_layers,\n",
    "                            dropout=dropout,\n",
    "                            highway_layers=highway_layers).to(device)\n",
    "        model.init_word_embeddings(embeddings.to(device))  # initialize embedding layer with pre-trained embeddings\n",
    "        model.fine_tune_word_embeddings(fine_tune_word_embeddings)  # fine-tune\n",
    "        optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n",
    "\n",
    "    # Loss functions\n",
    "    lm_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    crf_criterion = ViterbiLoss(tag_map).to(device)\n",
    "\n",
    "    # Since the language model's vocab is restricted to in-corpus indices, encode training/val with only these!\n",
    "    # word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n",
    "    # they were in the pre-trained embeddings\n",
    "    temp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\n",
    "    train_inputs = create_input_tensors(train_words, train_tags, temp_word_map, char_map,\n",
    "                                        tag_map)\n",
    "    val_inputs = create_input_tensors(val_words, val_tags, temp_word_map, char_map, tag_map)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=workers, pin_memory=False)\n",
    "    val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n",
    "                                             num_workers=workers, pin_memory=False)\n",
    "\n",
    "    # Viterbi decoder (to find accuracy during validation)\n",
    "    vb_decoder = ViterbiDecoder(tag_map)\n",
    "\n",
    "    # Epochs\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # One epoch's training\n",
    "        train(train_loader=train_loader,\n",
    "              model=model,\n",
    "              lm_criterion=lm_criterion,\n",
    "              crf_criterion=crf_criterion,\n",
    "              optimizer=optimizer,\n",
    "              epoch=epoch,\n",
    "              vb_decoder=vb_decoder)\n",
    "\n",
    "        # One epoch's validation\n",
    "        val_f1 = validate(val_loader=val_loader,\n",
    "                          model=model,\n",
    "                          crf_criterion=crf_criterion,\n",
    "                          vb_decoder=vb_decoder)\n",
    "\n",
    "        # Did validation F1 score improve?\n",
    "        is_best = val_f1 > best_f1\n",
    "        best_f1 = max(val_f1, best_f1)\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best)\n",
    "\n",
    "        # Decay learning rate every epoch\n",
    "        adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xEotsYyTj5R"
   },
   "outputs": [],
   "source": [
    "main_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smE9yMoQTzIE"
   },
   "source": [
    "Теперь запустим код из torchnlp, в котором мы на тех же данных будем обучать Transformer+CRF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psiMUkgHPHsd"
   },
   "outputs": [],
   "source": [
    "import torchnlp.ner as ner\n",
    "from torchnlp.tasks.sequence_tagging import TransformerTagger\n",
    "ner.train('ner-conll2003', TransformerTagger, ner.conll2003, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seminar_ner.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
