{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_sentiment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "z6ccN1AlFhNo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Классификация в задаче анализа тональности"
      ]
    },
    {
      "metadata": {
        "id": "oJ8_zAI8FhNp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Используя данные отзывов IMDB, построим модели лог-регресии, RNN и CNN для классификации документов на позитивный и негативный классы.\n",
        "\n",
        "Источник изложения: https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "metadata": {
        "id": "NHKLq9hEFhNr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В предположении, что PyTorch уже установлен, поставим дополнительные модули и загрузим модель для токенизации:"
      ]
    },
    {
      "metadata": {
        "id": "GgGxeuQjG9NI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#uncomment for linux or colab\n",
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5kiZro9eG-bG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a-WRgs-VFhNt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pB7uZ5L7FhN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLGeXcUjFhN8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3.6 -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FLGAJBxfFhN_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Загрузим датасет и получим из него выборку:"
      ]
    },
    {
      "metadata": {
        "id": "xsHAXknhFhOA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
        "\n",
        "train_src, test = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fk7MYqFNFhOG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Для начала обучим лог-регрессию на счётчиках слов:"
      ]
    },
    {
      "metadata": {
        "id": "9ZlrO6RwFhON",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vect = CountVectorizer()\n",
        "train_counters = vect.fit_transform([' '.join(sample.text) for sample in train_src])\n",
        "test_counters = vect.transform([' '.join(sample.text) for sample in test])\n",
        "\n",
        "_transform_label = lambda x: 1 if x == 'pos' else 0\n",
        "\n",
        "train_labels = [_transform_label(sample.label) for sample in train_src]\n",
        "test_labels = [_transform_label(sample.label) for sample in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SyTWDkhIFhOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(train_counters, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oe4zS8pkFhOS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "answers = model.predict(test_counters)\n",
        "print(f1_score(test_labels, answers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbdHPG-OFhOW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание 1:** Заменить оценивание при фиксированном пороге (по-умолчанию у метода predict он равен 0.5) на оценку по сетке порогов и посмотрите, при каком пороге достигается наилучший результат (здесь, в силу простоты задачи, он будет выражен слабо, но в реальности, при работе с несбалансированными выборками, это могут быть дестяки пунктов f1-меры). "
      ]
    },
    {
      "metadata": {
        "id": "VES0zfa8FhOX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8YV6wwUeFhOZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание 2:** Поскольку данные являются сбалансированными, допустимо использовать и простую метрику accuracy. Подсчитайте её."
      ]
    },
    {
      "metadata": {
        "id": "vrnpnLUzFhOa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lJHXnR-WFhOf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание 3:** Как правило, TF-IDF подходит в качестве признаков лучше, чем счётчики встречаемости. Замените признаковое пространство на TF-IDF, обучите модель и сравните качество."
      ]
    },
    {
      "metadata": {
        "id": "6QW4vPq0FhOg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yobndzKqFhOk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь попробуем обучить простую RNN на one-hot векторах слов. С учётом того, что в коллекции 100К уникальных слов, и векторы получаться достаточно громоздкие, урежем коллекцию до 25К слов, для всех прочих заведя токен unk (unknown). Кроме того, разобьём обучающий сет на обучение и валидацию для настройки параметров."
      ]
    },
    {
      "metadata": {
        "id": "6nuZQIq1FhOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "train, valid = train_src.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZsdwfpM5FhOp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train, max_size=25000)\n",
        "LABEL.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HM9FpclZFhOu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Выше в словаре, помимо отсечения 25К наиболее частых слов, также появятся два специальных токена - unk и pad (padding).\n",
        "\n",
        "Теперь создадим батчи, предварительно отсортировав тексты по длине, что минимизировать вставки pad-токенов:"
      ]
    },
    {
      "metadata": {
        "id": "DnHqmLZTFhOv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "St3MFtegFhOz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Определим саму модель, в которой однослойная RNN получает на вход one-hot, пропущенный через один Dense-слой:"
      ]
    },
    {
      "metadata": {
        "id": "CdNjtQ71FhO0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #x = [sent len, batch size]\n",
        "        embedded = self.embedding(x)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        assert torch.equal(output[-1, :, :], hidden.squeeze(0))\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dusAFp0rFhO3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P78tkqRKFhO6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Обучим созданную модель и оценим качество на валидационном сете. Сперва зададим оптимизатор и привяжем модель к GPU:"
      ]
    },
    {
      "metadata": {
        "id": "Ql_jSMPPFhO-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "23t5zm6LFhPC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Опишем функцию подсчёта accuracy, а также функции обучения и применения сети:"
      ]
    },
    {
      "metadata": {
        "id": "VQo4PuPzFhPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(F.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jepSRNlkFhPI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_func(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nb0KPBl8FhPL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_func(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i7lYMxI5FhPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Запустим и оценим качество на валидационном сете (по итерациям) и на тесте (в конце):"
      ]
    },
    {
      "metadata": {
        "id": "IyNdEv2VFhPQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 3\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train_func(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_func(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i2yU2HVvFhPT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_loss , test_acc = evaluate_func(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YY_6m1suFhPW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Как видим, качество низкое и не растёт с течением обучения. Попробуем улучшить модель:\n",
        "\n",
        "- заменим связку \"one-hot + dense\" на предобученные эмбеддинги\n",
        "- заменим RNN на BiLSTM\n",
        "- добавим дропаут\n",
        "- используем другой оптимизатор\n",
        "\n",
        "Если модель вылетает по памяти, можно попробовать а) начать обучению с неё (без обучения предыдущей) и б) уменьшить размер батча"
      ]
    },
    {
      "metadata": {
        "id": "hm187c4CFhPX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Начнём с подготовки данных:"
      ]
    },
    {
      "metadata": {
        "id": "By88bjwfFhPX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "\n",
        "BATCH_SIZEBATCH_S  = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYf3-MEoFhPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание 4:** Перепишите описанный выше класс RNN таким образом, чтобы в нём использовалась двунаправленная LSTM с параметром num_layers=n_layers и дропаут c заданной вероятностью dropout. Обратите внимание на то, что а) дропаут должен применяться как к весам входного слоя, так и к весам полносвязного на выходе; б) возвращаемое значение LSTM отличается от RNN наличием состояния (cell)."
      ]
    },
    {
      "metadata": {
        "id": "hIEreA47FhPc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        #put your code here\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #x = [sent len, batch size]\n",
        "        \n",
        "        #put your code here\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #put your code here\n",
        "        \n",
        "        #output = [sent len, batch size, hid dim * num directions]\n",
        "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
        "        #cell = [num layers * num directions, batch size, hid. dim]\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "\n",
        "        #hidden [batch size, hid. dim * num directions]            \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mbh1A1ncFhPf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15Nlnco0FhPi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Скопируем загруженные ранее эмбеддинги во входной слой модели, заменив веса инициализации:"
      ]
    },
    {
      "metadata": {
        "id": "ig18sFNaFhPi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MFHVkfspFhPl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Возьмём в качестве оптимизатора алгоритм Adam, всё остальное как и раньше:"
      ]
    },
    {
      "metadata": {
        "id": "CUNCkS16FhPm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-srwZAZRFhPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion  = nn.BCEWithLogitsLoss()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h-iKrpURFhPr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Обучим и оценим модель определёнными ранее функциями. Наличие model.train() и model.eval() в их телах обеспечит включение дропаута при обучение и выключение при оценивании."
      ]
    },
    {
      "metadata": {
        "id": "-r1oK48fFhPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 3\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train_func(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_func(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U466nkkPFhPy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_loss , test_acc = evaluate_func(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHEDOjrAFhP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Попробуем улучшить модель ещё сильнее, одновременно сократив количество параметров. Для это воспользуемся свёрточными сетями.\n",
        "\n",
        "Для создания свёрточного слоя воспользуемся nn.Conv2d, in_channels в нашем случае один (текст), out_channels -- это число число фильтров и размер ядер всех фильтров. Каждый фильтр будет иметь размерность [n x размерность эмбеддинга], где n - размер обрабатываемой n-граммы.\n",
        "\n",
        "В PyTorch RNN требуют, что размерность батча шла вторым измерением, а CNN - первым, поэтому нужно её переставить. Важно, что предложения имели длину не меньше размера самого большого из используемых фильтров (здесь это не страшно, поскольку в используемых данных нет текстов, состоящих из пяти и менее слов)."
      ]
    },
    {
      "metadata": {
        "id": "yb5g3czaFhP6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #x = [sent len, batch size]\n",
        "        x = x.permute(1, 0)\n",
        "                \n",
        "        #x = [batch size, sent len]\n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "        return self.fc(cat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_xPva9zFhP8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание 5:** Сейчас мы можем использовать только три различных фильтра, хотелось бы больше. Воспользуйтесь nn.ModuleList и перепишите класс сети для того, чтобы фильтров создавалось по количеству элементов в filter_sizes."
      ]
    },
    {
      "metadata": {
        "id": "MG4nFyJaqESM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        #put your code here\n",
        "        \n",
        "        def forward(self, x):\n",
        "            #put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_eWasooHFhQA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Как и раньше, создадим модель и загрузим предобученные эмбеддинги:\n"
      ]
    },
    {
      "metadata": {
        "id": "4Q_Yvs_gFhQB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pWlPRIqHFhQD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VnFgj1oRGl4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE  = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmWIaqIbFhQF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Используя определённые ранее функции, запустим обучение с оптимизатором Adam и оценим качество на валидации и тесте:"
      ]
    },
    {
      "metadata": {
        "id": "np-BTnydFhQF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XZC7S33pFhQH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train_func(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_func(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXQYQCLCFhQJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_loss , test_acc = evaluate_func(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}